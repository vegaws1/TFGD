
# ============================================================
# ONE-CLICK COLAB (UPDATED): TF-ARMA + TFGD (theory-consistent) + baselines
# Fixes PyTorch in-place autograd error via closed-form omega weights
# ============================================================

!pip -q install yfinance statsmodels arch seaborn tqdm

import os, sys, json, math, zipfile, warnings, platform
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

import torch
import torch.nn as nn

warnings.filterwarnings("ignore")
sns.set_style("whitegrid")

# ------------------------------------------------------------
# 0) Repro, device, folders, versions
# ------------------------------------------------------------
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

FIG_DIR = "/content/figs"
RES_DIR = "/content/results"
os.makedirs(FIG_DIR, exist_ok=True)
os.makedirs(RES_DIR, exist_ok=True)

import yfinance as yf
import statsmodels
import arch

env_info = {
    "python": sys.version,
    "platform": platform.platform(),
    "torch": torch.__version__,
    "numpy": np.__version__,
    "pandas": pd.__version__,
    "yfinance": yf.__version__,
    "statsmodels": statsmodels.__version__,
    "arch": arch.__version__,
    "device": str(device),
    "seed": SEED
}
with open(os.path.join(RES_DIR, "environment.json"), "w") as f:
    json.dump(env_info, f, indent=2)

print("Device:", device)
print("Environment saved:", os.path.join(RES_DIR, "environment.json"))

# ------------------------------------------------------------
# 1) Configuration
# ------------------------------------------------------------
DATASETS = [
    {"name": "SPY", "ticker": "SPY", "start": "2015-01-01"},
    {"name": "BTC", "ticker": "BTC-USD", "start": "2017-01-01"},
]

WF_INITIAL_TRAIN = 0.7
WF_HORIZON = 5
WF_STEP = 5

# TF-ARMA structure
P_AR = 5
Q_MA = 3

# TFGD hyperparams (optimizer)
TFGD_ALPHA = 0.7      # in (0,1)
TFGD_LAM = 0.3        # >0 tempering in optimizer
TFGD_LR = 0.01
TFGD_EPOCHS = 80
TFGD_M = None         # None -> ceil(5/TFGD_LAM)

# Backtest
COST = 0.0005
SLIPPAGE = 0.0002

# RNN baselines
LOOKBACK = 20
RNN_EPOCHS = 15
RNN_LR = 1e-3
RNN_HIDDEN = 32

# Optional: enable if you want detailed autograd traces (slower)
# torch.autograd.set_detect_anomaly(True)

# ------------------------------------------------------------
# 2) Data loader + preprocessing
# ------------------------------------------------------------
def load_yahoo_daily(ticker, start="2015-01-01", end=None):
    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)
    df = df.dropna()
    df["logret"] = np.log(df["Close"]).diff()
    df = df.dropna()
    return df

def walk_forward_splits(series, initial_train=0.7, horizon=5, step=5):
    n = len(series)
    train_end = int(initial_train * n)
    splits = []
    while train_end + horizon <= n:
        tr = np.arange(0, train_end)
        te = np.arange(train_end, train_end + horizon)
        splits.append((tr, te))
        train_end += step
    return splits

def standardize_train_only(train, test):
    mu, sigma = train.mean(), train.std() + 1e-12
    return (train - mu)/sigma, (test - mu)/sigma, mu, sigma

# ------------------------------------------------------------
# 3) Metrics + fold-based CI
# ------------------------------------------------------------
def metrics(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    mse = float(np.mean((y_true - y_pred)**2))
    mae = float(np.mean(np.abs(y_true - y_pred)))
    den = np.abs(y_true) + np.abs(y_pred) + 1e-12
    smape = float(100*np.mean(2*np.abs(y_pred - y_true)/den))
    da = float(100*np.mean(np.sign(y_true) == np.sign(y_pred)))
    return {"mse": mse, "mae": mae, "smape": smape, "da": da}

def fold_ci(fold_metrics, key="mse", alpha=0.05):
    vals = np.array([m[key] for m in fold_metrics], dtype=float)
    lo = np.quantile(vals, alpha/2)
    hi = np.quantile(vals, 1-alpha/2)
    return float(vals.mean()), float(lo), float(hi)

def summarize_with_ci(name, overall, fold_metrics):
    out = {"model": name, **overall}
    for k in ["mse","mae","smape","da"]:
        m, lo, hi = fold_ci(fold_metrics, k)
        out[f"{k}_mean_fold"] = m
        out[f"{k}_ci_lo"] = lo
        out[f"{k}_ci_hi"] = hi
    return out

# ------------------------------------------------------------
# 4) TFGD optimizer (theory-consistent weights)
#    w_j(alpha) = omega_j(-alpha) = Gamma(j+alpha)/(Gamma(alpha)Gamma(j+1))
#    recursion: w0=1, wj=((j-1+alpha)/j)*w_{j-1}
# ------------------------------------------------------------
from torch.optim.optimizer import Optimizer

class TFGD(Optimizer):
    """
    Tempered Fractional Gradient Descent (TFGD), truncated memory:
      theta <- theta - lr * sum_{j=0}^{min(k,M-1)} w_j(alpha)*exp(-lam*j)*g_{k-j}
    with w_j(alpha) = omega_j(-alpha) > 0
    """
    def __init__(self, params, lr=1e-2, alpha=0.7, lam=0.3, M=None, eps=1e-12):
        if lr <= 0: raise ValueError("lr must be > 0")
        if not (0 < alpha < 1): raise ValueError("alpha must be in (0,1)")
        if lam <= 0: raise ValueError("lam must be > 0")
        defaults = dict(lr=lr, alpha=alpha, lam=lam, M=M, eps=eps)
        super().__init__(params, defaults)
        self._initialized = False

    def _init_state(self):
        for group in self.param_groups:
            alpha = float(group["alpha"])
            lam = float(group["lam"])
            M = group["M"] if group["M"] is not None else int(math.ceil(5.0/lam))
            group["M"] = M

            w = torch.zeros(M, dtype=torch.float32)
            w[0] = 1.0
            for j in range(1, M):
                w[j] = ((j - 1 + alpha) / j) * w[j-1]

            group["w"] = w
            group["temp"] = torch.exp(-lam * torch.arange(M, dtype=torch.float32))

            for p in group["params"]:
                self.state[p]["g_hist"] = []

        self._initialized = True

    @torch.no_grad()
    def step(self, closure=None):
        if not self._initialized:
            self._init_state()

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            lr, M = group["lr"], group["M"]
            w = group["w"].to(next(iter(group["params"])).device)
            temp = group["temp"].to(next(iter(group["params"])).device)

            for p in group["params"]:
                if p.grad is None:
                    continue
                g = p.grad.detach()
                gh = self.state[p]["g_hist"]
                gh.insert(0, g.clone())
                if len(gh) > M:
                    gh.pop()

                v = torch.zeros_like(g)
                for j in range(len(gh)):
                    v.add_(w[j] * temp[j] * gh[j])

                p.add_(-lr * v)

        return loss

# ------------------------------------------------------------
# 5) Theory validation: alignment coefficient check
# ------------------------------------------------------------
def tfgd_alignment_check(alpha=0.7, lam=0.3, J=4000):
    w = np.zeros(J)
    w[0] = 1.0
    for j in range(1, J):
        w[j] = ((j - 1 + alpha) / j) * w[j-1]
    s = np.sum(w * np.exp(-lam*np.arange(J)))
    closed = (1 - np.exp(-lam))**(-alpha)
    return s, closed, abs(s-closed)/closed

# ------------------------------------------------------------
# 6) TF-ARMA model (UPDATED, autograd-safe)
#    weights computed via closed-form using torch.lgamma (NO in-place)
# ------------------------------------------------------------
class TFARMA(nn.Module):
    def __init__(self, p=5, q=3):
        super().__init__()
        self.p, self.q = int(p), int(q)

        # mu, nu in (0,1)
        self.mu_raw = nn.Parameter(torch.tensor(0.0))
        self.nu_raw = nn.Parameter(torch.tensor(0.0))

        # positive parameters via exp
        self.lam_scale_raw = nn.Parameter(torch.tensor(0.0))    # lambda > 0
        self.lam_eps_raw   = nn.Parameter(torch.tensor(-1.0))   # >0
        self.lamY_raw      = nn.Parameter(torch.tensor(-1.0))   # >0

    def mu(self):  return torch.sigmoid(self.mu_raw)
    def nu(self):  return torch.sigmoid(self.nu_raw)
    def lam_scale(self): return torch.exp(self.lam_scale_raw) + 1e-12
    def lam_eps(self):   return torch.exp(self.lam_eps_raw)   + 1e-12
    def lamY(self):      return torch.exp(self.lamY_raw)      + 1e-12

    def omega_minus_alpha(self, alpha, L, dtype):
        """
        omega_k(-alpha) = Gamma(k+alpha)/(Gamma(alpha)Gamma(k+1)), k=0..L-1
        computed in stable closed-form using lgamma (autograd-safe).
        """
        k = torch.arange(L, device=alpha.device, dtype=dtype)
        logw = torch.lgamma(k + alpha) - torch.lgamma(alpha) - torch.lgamma(k + 1.0)
        return torch.exp(logw)

    def forward_recursive(self, y):
        """
        y: (T,) tensor
        returns: yhat, eps, start, (w_ma, w_ar)
        """
        T = y.shape[0]
        p, q = self.p, self.q
        start = max(p, q) + 1
        dtype = y.dtype

        mu = self.mu()
        nu = self.nu()
        lam_scale = self.lam_scale()
        lam_eps   = self.lam_eps()
        lamY      = self.lamY()

        w_ma = self.omega_minus_alpha(mu, q+1, dtype) * torch.exp(-lam_eps * torch.arange(q+1, device=y.device, dtype=dtype))
        w_ar = self.omega_minus_alpha(nu, p+1, dtype) * torch.exp(-lamY  * torch.arange(p+1, device=y.device, dtype=dtype))

        eps_list, yhat_list = [], []
        z = torch.zeros((), device=y.device, dtype=dtype)
        for _ in range(start):
            eps_list.append(z)
            yhat_list.append(z)

        for t in range(start, T):
            y_lags = y[t-p:t]                    # (p,)
            e_lags = torch.stack(eps_list[t-q:t])# (q,)

            ar = (w_ar[1:].flip(0) * y_lags).sum()
            ma = lam_scale * (w_ma[1:].flip(0) * e_lags).sum()  # E[eps_t]=0 in training is NOT assumed; we use past eps only
            yhat_t = ma - ar
            eps_t  = y[t] - yhat_t

            yhat_list.append(yhat_t)
            eps_list.append(eps_t)

        yhat = torch.stack(yhat_list)
        eps  = torch.stack(eps_list)
        return yhat, eps, start, (w_ma.detach(), w_ar.detach())

    @torch.no_grad()
    def forecast_block(self, y_train, h=5):
        """
        Multi-step forecast: assume E[future eps]=0 (standard ARMA forecast).
        """
        p, q = self.p, self.q
        dtype = y_train.dtype
        yhat, eps, start, _ = self.forward_recursive(y_train)

        mu = self.mu()
        nu = self.nu()
        lam_scale = self.lam_scale()
        lam_eps   = self.lam_eps()
        lamY      = self.lamY()

        w_ma = self.omega_minus_alpha(mu, q+1, dtype) * torch.exp(-lam_eps * torch.arange(q+1, device=y_train.device, dtype=dtype))
        w_ar = self.omega_minus_alpha(nu, p+1, dtype) * torch.exp(-lamY  * torch.arange(p+1, device=y_train.device, dtype=dtype))

        y_hist = list(y_train.detach().cpu().numpy())
        e_hist = list(eps.detach().cpu().numpy())

        preds = []
        for _ in range(h):
            y_lags = torch.tensor(y_hist[-p:], dtype=dtype, device=y_train.device)
            e_lags = torch.tensor(e_hist[-q:], dtype=dtype, device=y_train.device)

            ar = (w_ar[1:].flip(0) * y_lags).sum()
            ma = lam_scale * (w_ma[1:].flip(0) * e_lags).sum()
            y_next = (ma - ar).detach()

            preds.append(y_next)
            y_hist.append(float(y_next.cpu().item()))
            e_hist.append(0.0)  # expected future innovation = 0

        return torch.stack(preds)

# ------------------------------------------------------------
# 7) Train / Eval TF-ARMA + TFGD (walk-forward)
# ------------------------------------------------------------
def train_tfarma_tfgd(y_train_s, p=5, q=3, epochs=80, lr=0.01, alpha=0.7, lam=0.3, M=None):
    y = torch.tensor(y_train_s, dtype=torch.float32, device=device)
    model = TFARMA(p=p, q=q).to(device)
    opt = TFGD(model.parameters(), lr=lr, alpha=alpha, lam=lam, M=M)

    losses = []
    for _ in range(epochs):
        opt.zero_grad()
        yhat, eps, start, _ = model.forward_recursive(y)
        loss = ((y[start:] - yhat[start:])**2).mean()
        loss.backward()
        opt.step()
        losses.append(float(loss.detach().cpu().item()))
    return model, losses

def eval_tfarma_walkforward(series, splits, p=5, q=3, epochs=80, lr=0.01, alpha=0.7, lam=0.3, M=None):
    y_true_all, y_pred_all = [], []
    fold_metrics = []
    loss_curves = []

    for (tr, te) in tqdm(splits, desc="Walk-forward TF-ARMA+TFGD"):
        y_train = series[tr]
        y_test  = series[te]

        y_train_s, y_test_s, mu, sigma = standardize_train_only(y_train, y_test)
        model, losses = train_tfarma_tfgd(y_train_s, p=p, q=q, epochs=epochs, lr=lr, alpha=alpha, lam=lam, M=M)
        loss_curves.append(losses)

        with torch.no_grad():
            y_tr_t = torch.tensor(y_train_s, dtype=torch.float32, device=device)
            preds_s = model.forecast_block(y_tr_t, h=len(te)).cpu().numpy()

        preds = preds_s * sigma + mu
        y_true_all.extend(y_test)
        y_pred_all.extend(preds)
        fold_metrics.append(metrics(y_test, preds))

    overall = metrics(y_true_all, y_pred_all)
    return overall, fold_metrics, np.array(y_true_all), np.array(y_pred_all), loss_curves

# ------------------------------------------------------------
# 8) Baselines: ARIMA fast, AR(1)-GARCH, LSTM, GRU
# ------------------------------------------------------------
from statsmodels.tsa.statespace.sarimax import SARIMAX
from arch import arch_model

def arima_fast_walkforward(series, splits, order=(1,0,1), maxiter=200):
    y_true_all, y_pred_all = [], []
    fold_metrics = []

    tr0, te0 = splits[0]
    train_end = len(tr0)
    y0 = series[:train_end]

    model = SARIMAX(y0, order=order, trend="n",
                    enforce_stationarity=False, enforce_invertibility=False)
    res = model.fit(disp=False, maxiter=maxiter)

    current_end = train_end
    for (tr, te) in tqdm(splits, desc="Walk-forward ARIMA (FAST)"):
        desired_train_end = len(tr)
        if desired_train_end > current_end:
            res = res.append(series[current_end:desired_train_end], refit=False)
            current_end = desired_train_end

        h = len(te)
        pred = np.asarray(res.forecast(steps=h))
        y_test = series[te]

        y_true_all.extend(y_test)
        y_pred_all.extend(pred)
        fold_metrics.append(metrics(y_test, pred))

        res = res.append(y_test, refit=False)
        current_end = desired_train_end + h

    overall = metrics(y_true_all, y_pred_all)
    return overall, fold_metrics, np.array(y_true_all), np.array(y_pred_all)

def ar1_garch_walkforward(series, splits):
    y_true_all, y_pred_all = [], []
    fold_metrics = []
    for (tr, te) in tqdm(splits, desc="Walk-forward AR(1)-GARCH"):
        y_train, y_test = series[tr], series[te]
        train_scaled = y_train * 100.0
        try:
            am = arch_model(train_scaled, mean="AR", lags=1, vol="GARCH", p=1, q=1, dist="normal")
            res = am.fit(disp="off")
            f = res.forecast(horizon=len(te), reindex=False)
            pred_scaled = f.mean.values[-1]
            pred = pred_scaled / 100.0
        except Exception:
            pred = np.zeros(len(te))

        y_true_all.extend(y_test)
        y_pred_all.extend(pred)
        fold_metrics.append(metrics(y_test, pred))

    overall = metrics(y_true_all, y_pred_all)
    return overall, fold_metrics, np.array(y_true_all), np.array(y_pred_all)

class LSTMModel(nn.Module):
    def __init__(self, hidden=32, layers=1):
        super().__init__()
        self.rnn = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=layers, batch_first=True)
        self.fc = nn.Linear(hidden, 1)
    def forward(self, x):
        out, _ = self.rnn(x)
        return self.fc(out[:, -1, :])

class GRUModel(nn.Module):
    def __init__(self, hidden=32, layers=1):
        super().__init__()
        self.rnn = nn.GRU(input_size=1, hidden_size=hidden, num_layers=layers, batch_first=True)
        self.fc = nn.Linear(hidden, 1)
    def forward(self, x):
        out, _ = self.rnn(x)
        return self.fc(out[:, -1, :])

def make_sequences(x, lookback=20):
    X, Y = [], []
    for i in range(lookback, len(x)):
        X.append(x[i-lookback:i])
        Y.append(x[i])
    X = np.array(X)[:, :, None]
    Y = np.array(Y)[:, None]
    return X, Y

def train_rnn(model, X_train, y_train, epochs=15, lr=1e-3):
    model = model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()
    Xt = torch.tensor(X_train, dtype=torch.float32, device=device)
    yt = torch.tensor(y_train, dtype=torch.float32, device=device)
    for _ in range(epochs):
        opt.zero_grad()
        pred = model(Xt)
        loss = loss_fn(pred, yt)
        loss.backward()
        opt.step()
    return model

def rnn_walkforward(series, splits, model_type="LSTM", lookback=20, epochs=15, lr=1e-3, hidden=32):
    y_true_all, y_pred_all = [], []
    fold_metrics = []

    for (tr, te) in tqdm(splits, desc=f"Walk-forward {model_type}"):
        y_train, y_test = series[tr], series[te]
        y_train_s, y_test_s, mu, sigma = standardize_train_only(y_train, y_test)

        Xtr, Ytr = make_sequences(y_train_s, lookback=lookback)
        model = LSTMModel(hidden=hidden) if model_type=="LSTM" else GRUModel(hidden=hidden)
        model = train_rnn(model, Xtr, Ytr, epochs=epochs, lr=lr)

        history = list(y_train_s.copy())
        preds_s = []
        with torch.no_grad():
            for _ in range(len(te)):
                x_in = np.array(history[-lookback:])[None, :, None]
                x_in = torch.tensor(x_in, dtype=torch.float32, device=device)
                p = model(x_in).cpu().numpy().ravel()[0]
                preds_s.append(p)
                history.append(p)

        preds = np.array(preds_s) * sigma + mu
        y_true_all.extend(y_test)
        y_pred_all.extend(preds)
        fold_metrics.append(metrics(y_test, preds))

    overall = metrics(y_true_all, y_pred_all)
    return overall, fold_metrics, np.array(y_true_all), np.array(y_pred_all)

# ------------------------------------------------------------
# 9) Backtest OOS (sign strategy + costs)
# ------------------------------------------------------------
def backtest_sign_strategy(y_true, y_pred, cost=0.0005, slippage=0.0002):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    pos = np.sign(y_pred)
    pos[pos==0] = 1
    turnover = np.abs(np.diff(pos, prepend=pos[0]))
    net = pos*y_true - turnover*(cost + slippage)
    equity = np.cumprod(1 + net)
    dd = 1 - equity/np.maximum.accumulate(equity)
    max_dd = float(np.max(dd))
    ann_ret = float((equity[-1]**(252/len(net)) - 1)*100) if len(net) > 20 else float((equity[-1]-1)*100)
    sharpe = float(np.mean(net)/(np.std(net)+1e-12)*np.sqrt(252))
    return {"annual_return_%": ann_ret, "max_drawdown_%": max_dd*100, "sharpe": sharpe, "equity": equity}

# ------------------------------------------------------------
# 10) Run theory check + experiments
# ------------------------------------------------------------
s_num, s_closed, relerr = tfgd_alignment_check(alpha=TFGD_ALPHA, lam=TFGD_LAM, J=4000)
print("\n[Theory check] TFGD alignment coefficient:")
print("  numerical sum =", s_num)
print("  closed-form   =", s_closed)
print("  relative err  =", relerr)

# plot weights (first 60)
Jplot = 60
w = np.zeros(Jplot); w[0]=1.0
for j in range(1, Jplot):
    w[j]=((j-1+TFGD_ALPHA)/j)*w[j-1]
temp = np.exp(-TFGD_LAM*np.arange(Jplot))

plt.figure(figsize=(10,4))
plt.plot(w, label="w_j = omega_j(-alpha)")
plt.plot(w*temp, label="w_j * exp(-lam j)")
plt.title("TFGD weights and tempered weights (first 60)")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(FIG_DIR, "tfgd_weights.png"), dpi=200)
plt.close()

all_results = []

for ds in DATASETS:
    name, ticker, start = ds["name"], ds["ticker"], ds["start"]
    df = load_yahoo_daily(ticker, start=start)
    series = df["logret"].values.astype(float)
    splits = walk_forward_splits(series, initial_train=WF_INITIAL_TRAIN, horizon=WF_HORIZON, step=WF_STEP)

    print(f"\n===== Dataset {name} ({ticker}) =====")
    print("Samples:", len(series), "Folds:", len(splits), "Horizon:", WF_HORIZON, "Step:", WF_STEP)

    # TF-ARMA + TFGD
    tf_overall, tf_folds, tf_y, tf_p, tf_losses = eval_tfarma_walkforward(
        series, splits, p=P_AR, q=Q_MA,
        epochs=TFGD_EPOCHS, lr=TFGD_LR, alpha=TFGD_ALPHA, lam=TFGD_LAM, M=TFGD_M
    )

    # ARIMA fast
    ar_overall, ar_folds, ar_y, ar_p = arima_fast_walkforward(series, splits, order=(1,0,1))

    # AR(1)-GARCH
    ga_overall, ga_folds, ga_y, ga_p = ar1_garch_walkforward(series, splits)

    # LSTM / GRU
    lstm_overall, lstm_folds, lstm_y, lstm_p = rnn_walkforward(
        series, splits, model_type="LSTM", lookback=LOOKBACK, epochs=RNN_EPOCHS, lr=RNN_LR, hidden=RNN_HIDDEN
    )
    gru_overall, gru_folds, gru_y, gru_p = rnn_walkforward(
        series, splits, model_type="GRU", lookback=LOOKBACK, epochs=RNN_EPOCHS, lr=RNN_LR, hidden=RNN_HIDDEN
    )

    summaries = [
        summarize_with_ci(f"{name}: TF-ARMA+TFGD", tf_overall, tf_folds),
        summarize_with_ci(f"{name}: ARIMA(1,0,1)", ar_overall, ar_folds),
        summarize_with_ci(f"{name}: AR(1)-GARCH(1,1)", ga_overall, ga_folds),
        summarize_with_ci(f"{name}: LSTM", lstm_overall, lstm_folds),
        summarize_with_ci(f"{name}: GRU", gru_overall, gru_folds),
    ]
    df_sum = pd.DataFrame(summaries)
    all_results.append(df_sum)
    df_sum.to_csv(os.path.join(RES_DIR, f"summary_{name}.csv"), index=False)

    # plots
    def plot_pred(y_true, y_pred, title, fname, n=200):
        plt.figure(figsize=(12,4))
        plt.plot(y_true[:n], label="True", linewidth=2)
        plt.plot(y_pred[:n], label="Pred", linewidth=1.5)
        plt.title(title)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(FIG_DIR, fname), dpi=200)
        plt.close()

    plot_pred(tf_y, tf_p, f"{name} TF-ARMA+TFGD OOS", f"{name}_tfarma_pred.png")
    plot_pred(ar_y, ar_p, f"{name} ARIMA OOS", f"{name}_arima_pred.png")

    if len(tf_losses) > 0:
        mid = len(tf_losses)//2
        plt.figure(figsize=(8,4))
        plt.plot(tf_losses[mid])
        plt.title(f"{name} TF-ARMA+TFGD Training Loss (one fold)")
        plt.tight_layout()
        plt.savefig(os.path.join(FIG_DIR, f"{name}_tfarma_loss.png"), dpi=200)
        plt.close()

    bt = backtest_sign_strategy(tf_y, tf_p, cost=COST, slippage=SLIPPAGE)
    with open(os.path.join(RES_DIR, f"backtest_{name}_tfarma.json"), "w") as f:
        json.dump({k:v for k,v in bt.items() if k!="equity"}, f, indent=2)

    plt.figure(figsize=(10,4))
    plt.plot(bt["equity"])
    plt.title(f"{name} TF-ARMA+TFGD OOS Equity (costs)")
    plt.tight_layout()
    plt.savefig(os.path.join(FIG_DIR, f"{name}_equity_tf.png"), dpi=200)
    plt.close()

    print("\nSummary (key metrics):")
    print(df_sum[["model","mse","mae","smape","da"]])

# concat results
df_all = pd.concat(all_results, axis=0, ignore_index=True)
df_all.to_csv(os.path.join(RES_DIR, "summary_ALL.csv"), index=False)
with open(os.path.join(RES_DIR, "summary_ALL.json"), "w") as f:
    json.dump(df_all.to_dict(orient="records"), f, indent=2)

plt.figure(figsize=(12,4))
sns.barplot(data=df_all, x="model", y="mse")
plt.xticks(rotation=20, ha="right")
plt.title("OOS MSE comparison (all datasets/models)")
plt.tight_layout()
plt.savefig(os.path.join(FIG_DIR, "mse_all_models.png"), dpi=200)
plt.close()

# export zip
zip_path = "/content/tfarma_artifacts.zip"
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for folder in [FIG_DIR, RES_DIR]:
        for root, _, files in os.walk(folder):
            for fn in files:
                fp = os.path.join(root, fn)
                arc = os.path.relpath(fp, "/content")
                z.write(fp, arcname=arc)

print("\nâœ… DONE. Download:", zip_path)
print("Contains:", FIG_DIR, "and", RES_DIR)
